% !TeX spellcheck = en_US
% !TeX encoding = utf8
% !TeX program = xelatex
% !BIB program = bibtex
% \documentclass[mathserif,compress,12pt]{ctexbeamer}
\documentclass[12pt,notes,mathserif]{beamer}
% \documentclass[draft]{beamer}	
\usetheme{Singapore}
% \usetheme{Hannover}
%\usepackage{pgfpages}
%\setbeameroption{show notes on second screen}

\usepackage[british]{babel}
\usepackage{graphicx,hyperref,url}
% \usepackage{ru}
\usepackage{mmstyles,bm,ulem}

\usepackage{listings}
\usefonttheme[onlymath]{serif}
\usepackage{fontspec}
\usepackage{xeCJK}
% \pgfdeclareimage[width=\paperwidth,height=\paperheight]{bg}{background}
% \setbeamertemplate{background}{\pgfuseimage{bg}}
%% columns
\newcommand{\begincols}[1]{\begin{columns}{#1}}
\newcommand{\stopcols}{\end{columns}}
% \usepackage[backend=biber]{biblatex}
% \bibliography{./ref.bib}
%\addbibresource{ref.bib}
\usepackage{indentfirst}
\usepackage{longtable}
\usepackage{float}
%\usepackage{picins}
\usepackage{rotating}
\usepackage{subfigure}
\usepackage{tabu}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{amsfonts}
\usepackage{appendix}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{geometry}
% \setCJKfamilyfont{cjkhwxk}{SimSun}
% \newcommand*{\cjkhwxk}{\CJKfamily{cjkhwxk}}
%\newfontfamily{\consolas}{Consolas}
%\newfontfamily{\monaco}{Monaco}
%\setmonofont[Mapping={}]{Consolas}	%Ëã±ÊñáÂºïÂè∑‰πãÁ±ªÁöÑÊ≠£Â∏∏ÊòæÁ§∫ÔºåÁõ∏ÂΩì‰∫éËÆæÁΩÆËã±ÊñáÂ≠ó‰Ωì
%\setsansfont{Consolas} %ËÆæÁΩÆËã±ÊñáÂ≠ó‰Ωì Monaco, Consolas,  Fantasque Sans Mono
% \setmainfont{Times New Roman}
% \newfontfamily{\consolas}{Times New Roman}
% \newfontfamily{\monaco}{Arial}
% \setCJKmainfont{Times New Roman}
%\setmainfont{MONACO.TTF}
%\setsansfont{MONACO.TTF}
\newcommand{\verylarge}{\fontsize{60pt}{\baselineskip}\selectfont}  
\newcommand{\chuhao}{\fontsize{44.9pt}{\baselineskip}\selectfont}  
\newcommand{\xiaochu}{\fontsize{38.5pt}{\baselineskip}\selectfont}  
\newcommand{\yihao}{\fontsize{27.8pt}{\baselineskip}\selectfont}  
\newcommand{\xiaoyi}{\fontsize{25.7pt}{\baselineskip}\selectfont}  
\newcommand{\erhao}{\fontsize{23.5pt}{\baselineskip}\selectfont}  
\newcommand{\xiaoerhao}{\fontsize{19.3pt}{\baselineskip}\selectfont} 
\newcommand{\sihao}{\fontsize{14pt}{\baselineskip}\selectfont}      % Â≠óÂè∑ËÆæÁΩÆ  
\newcommand{\xiaosihao}{\fontsize{12pt}{\baselineskip}\selectfont}  % Â≠óÂè∑ËÆæÁΩÆ  
\newcommand{\wuhao}{\fontsize{10.5pt}{\baselineskip}\selectfont}    % Â≠óÂè∑ËÆæÁΩÆ  
\newcommand{\xiaowuhao}{\fontsize{9pt}{\baselineskip}\selectfont}   % Â≠óÂè∑ËÆæÁΩÆ  
\newcommand{\liuhao}{\fontsize{7.875pt}{\baselineskip}\selectfont}  % Â≠óÂè∑ËÆæÁΩÆ  
\newcommand{\qihao}{\fontsize{5.25pt}{\baselineskip}\selectfont}    % Â≠óÂè∑ËÆæÁΩÆ 

\graphicspath{{./fig/}}

% \setbeamertemplate{footnote}{%
%   \hangpara{2em}{1}%
%   \makebox[2em][l]{\insertfootnotemark}\footnotesize\insertfootnotetext\par%
% }

\definecolor{cred}{rgb}{0.6,0,0}
\definecolor{cgreen}{rgb}{0.25,0.5,0.35}
\definecolor{cpurple}{rgb}{0.5,0,0.35}
\definecolor{cdocblue}{rgb}{0.25,0.35,0.75}
\definecolor{cdark}{rgb}{0.95,1.0,1.0}
\lstset{
	language=R,
	numbers=left,
	numberstyle=\tiny\color{black},
	keywordstyle=\color{cpurple}\consolas,
	commentstyle=\color{cgreen}\consolas,
	stringstyle=\color{cred}\consolas,
	frame=single,
	escapeinside=``,
	xleftmargin=1em,
	xrightmargin=1em, 
	backgroundcolor=\color{cdark},
	aboveskip=1em,
	breaklines=true,
	tabsize=3
} 

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

  
% The title of the presentation:
%  - first a short version which is visible at the bottom of each slide;
%  - second the full title shown on the title slide;
% \title[]{\LARGE CSE 5526: Introduction to Neural Networks}
\title{Support Vector Machines (SVM)}

% Optional: a subtitle to be dispalyed on the title slide
% \subtitle{\Large Support Vector Machines
% (SVM)}

% The author(s) of the presentation:
%  - again first a short version to be displayed at the bottom;
%  - next the full list of authors, which may include contact information;
\author[YingmingLi]{Yingming Li \\ yingming@zju.edu.cn}
% The institute:
%  - to start the name of the university as displayed on the top of each slide
%    this can be adjusted such that you can also create a Dutch version
%  - next the institute information as displayed on the title slide

\institute[DSERC, ZJU]{Data Science \& Engineering Research Center, ZJU}
% Add a date and possibly the name of the event to the slides
%  - again first a short version to be shown at the bottom of each slide
%  - second the full date and event name for the title slide
\date[\today]{\today}
\begin{document}

\AtBeginSection[]
{
	\begin{frame}
		\frametitle{Outline}
		\tableofcontents[currentsection]
	\end{frame}
}

% \AtBeginSubsection[2-]
% {
%    \begin{frame}
%        \frametitle{Outline}
%        \tableofcontents[currentsection]
%    \end{frame}
% }
\begin{frame}[c]
	\titlepage
	% \begin{center}
		% MLP Tips
	% \end{center}
\end{frame}

% 2
\begin{frame}[c]
\frametitle{Perceptrons find any separating hyperplane}
\begin{center}
Depends on initialization and ordering of training points\\
\includegraphics[width=0.7\linewidth]{fig8/lec82.jpg}
\end{center}
\end{frame}
% 3
\begin{frame}[c]
\frametitle{Perceptrons find any separating hyperplane}
\begin{center}
Depends on initialization and ordering of training points\\
\includegraphics[width=0.7\linewidth]{fig8/lec83.jpg}
\end{center}
\end{frame}

% 4
\begin{frame}[c]
\frametitle{Perceptrons find any separating hyperplane}
\begin{center}
Depends on initialization and ordering of training points\\
\includegraphics[width=0.7\linewidth]{fig8/lec84.jpg}
\end{center}
\end{frame}

% 5
\begin{frame}[c]
\frametitle{Perceptrons find any separating hyperplane}
\begin{center}
Depends on initialization and ordering of training points\\
\includegraphics[width=0.7\linewidth]{fig8/lec85.jpg}
\end{center}
\end{frame}



% 6
\begin{frame}[c]
\frametitle{But the maximum margin hyperplane generalizes the best to new data}
\begin{center}
According to computational/statistical learning theory\\
\includegraphics[width=0.6\linewidth]{fig8/lec86.jpg}
\end{center}
\end{frame}


% 7
\begin{frame}[c]
\frametitle{But the maximum margin hyperplane generalizes the best to new data}
\begin{itemize}
\item According to computational learning theory
\item Also known as statistical learning theory
\item We won't get into the details of that
\item Recall from the perceptron convergence proof
		\begin{itemize}
		\item We assumed the existance of a best hyperplane $\mathbf{w}_0$
		\item Which provided the maximum margin ${\alpha}$
		\item Such that $d_p\mathbf{w}_0^T\mathbf{x}_p\leqslant{}\alpha$ for all training points $p$
		\end{itemize}
\item The SVM actually finds this hyperplane
\end{itemize}

\end{frame}



% 8
\begin{frame}[c]
\frametitle{The maximum margin only depends on certain points, the support vectors}
\begin{center}
\includegraphics[width=0.7\linewidth]{fig8/lec88.jpg}
\end{center}
\end{frame}

% 9
\begin{frame}[c]
\frametitle{The maximum margin only depends on certain points, the support vectors}
\begin{center}
\includegraphics[width=0.7\linewidth]{fig8/lec89.jpg}
\end{center}
\end{frame}


% 10
\begin{frame}[c]
\frametitle{The maximum margin only depends on certain points, the support vectors}
\begin{center}
\includegraphics[width=0.7\linewidth]{fig8/lec810.jpg}
\end{center}
\end{frame}


% 11
\begin{frame}[c]
\frametitle{Maximum margin problem}
\begin{itemize}
\item  Given a set of data from two classes $\{\mathbf{x}_p,d_p\}$
		\begin{itemize}
		\item $\vx_p\in \mathbb{R}^D$ and $d_p\in\{-1,1\}$
		\item Assume the classes are linearly separable for now
		\end{itemize}
\item Find the hyperplane that separates them
		\begin{itemize}
		\item with maximum margin
		\end{itemize}
\item Equation of general linear discriminant function
\[y(\mathbf{x})=\mathbf{w}^T\mathbf{x}+b\]

\item Find $\mathbf{w}$ and $b$ that give maximum margin
\item How can we quantify margin?
\end{itemize}

\end{frame}



% 12
\begin{frame}[c]
\frametitle{$\mathbf{w}$ is perpendicular to the hyperplane, $b$ defines its distance from the origin}
\begin{center}
\includegraphics[width=0.7\linewidth]{fig8/lec812.jpg}
\end{center}
\end{frame}


% 13
\begin{frame}[c]
\frametitle{$\mathbf{w}$ is perpendicular to the hyperplane}
\begin{itemize}
\item Consider two points on the hyperplane $\mathbf{x}_A$ and $\mathbf{x}_B$
\item Then $y(\mathbf{x}_A)=y(\mathbf{x}_B)=0$ by definition
\item So $0 =y(\mathbf{x}_A)‚àí y(\mathbf{x}_B) = \mathbf{w}^T(\mathbf{x}_A-\mathbf{x}_B)$ùíôùíô ùêµùêµ 
\item $\mathbf{x}_A-\mathbf{x}_B$ is  a vector pointing along the hyperplane
\item So $\mathbf{w}$ is perpendicular to the hyperplane
\end{itemize}

\end{frame}





% 14
\begin{frame}[c]
\frametitle{$b$ defines the hyerplane's distance from the origin}
\begin{itemize}
\item Consider a general point $\mathbf{x}$
\item Its distance to the origin is $D=\dfrac{\mathbf{w}^Tx}{||\mathbf{w}||}$
\item If $\mathbf{x}$ is on the hyperplane, then $y(\mathbf{x})=0$
		\begin{itemize}
		\item So $\mathbf{w}^T x=-b$
		\end{itemize}
\item So the distance from the hyperplane to the origin is
\[
D=-\dfrac{b}{||\mathbf{w}||}
\]
\end{itemize}

\end{frame}



% 15
\begin{frame}[c]
\frametitle{$\mathbf{w}$ is perpendicular to the hyperplane, $b$ defines its distance from the origin}
\begin{center}
\includegraphics[width=0.7\linewidth]{fig8/lec815.jpg}
\end{center}
\end{frame}


% 16
\begin{frame}[c]
\frametitle{The distance from point $\mathbf{x}$ùíôto the hyperplane is ùë¶$y(\mathbf{x})/||\mathbf{w}||$}
\begin{center}
\includegraphics[width=0.7\linewidth]{fig8/lec816.jpg}
\end{center}
\end{frame}



% 17
\begin{frame}[c]
\frametitle{The distance from point $\mathbf{x}$ùíôto the hyperplane is ùë¶$y(\mathbf{x})/||\mathbf{w}||$}
\begin{itemize}
\item Consider a point $\mathbf{x}$ and its projection onto the hyperplane $\mathbf{x}_{\bot}$ so that $\mathbf{x}=\mathbf{x}_{\bot}+r\dfrac{\mathbf{w}}{||\mathbf{w}||}$
\item We want to find $r$, the distance to the hyperplane
\item Multiply both sides by $\mathbf{w}^T$ and add $b$
\[
\mathbf{w}^T\mathbf{x}+b=
\mathbf{w}^T\mathbf{x}_{\bot} +b+r\dfrac{||\mathbf{w}||^2}{||\mathbf{w}||} 
\]
\[
y(\mathbf{x})=y(\mathbf{x}_{\bot})+r||\mathbf{w}||
\]
\[
r=\dfrac{y(\mathbf{x})}{||\mathbf{w}||}
\]
\end{itemize}
\end{frame}



% 18
\begin{frame}[c]
\frametitle{The distance from point $\mathbf{x}$ùíôto the hyperplane is $y(\mathbf{x})/||\mathbf{w}||$}
\begin{center}
\includegraphics[width=0.7\linewidth]{fig8/lec818.jpg}
\end{center}
\end{frame}


% 19
\begin{frame}[c]
\frametitle{The maximum margin hyperplane is farthest from all of the data points}
\begin{center}
\includegraphics[width=0.7\linewidth]{fig8/lec819.jpg}
\end{center}
\end{frame}


% 20
\begin{frame}[c]
\frametitle{The maximum margin hyperplane is farthest from all of the data points}
\begin{itemize}
\item The margin is defined as
\[
\begin{array}{r@{~}l}
\alpha=&\min_pd_p\dfrac{y(\mathbf{x}_p)}{||\mathbf{w}||}\\
=&\dfrac{1}{||\mathbf{w}||}\min_pd_py(\mathbf{x}_p)
\end{array}
\]
\item We want to find $\mathbf{w}$ and  that maximize the margin $\argmax_{\mathbf{w},b}\dfrac{1}{||\mathbf{w}||} \min_pd_py(\mathbf{x}_p)$
\item Solving this problem is hard as it is written
\end{itemize}
\end{frame}

% 21
\begin{frame}[c]
\frametitle{We are free to choose a rescaling of $\mathbf{w}$}
\begin{itemize}
\item If we replace $\mathbf{w}$ by $a\mathbf{w}$ and $b$ with $ab$
\item Then the margin is unchanged
\[
\min\nolimits_pd_p\dfrac{a\mathbf{w}^T\mathbf{x}_p+ab}{a||\mathbf{w}||}=
\min\nolimits_pd_p
\dfrac{\mathbf{w}^T\mathbf{x}_p+b}{||\mathbf{w}||}
\]
\item So choose $a$ such that $\min_pd_p(a\mathbf{w}^T\mathbf{x}_p+ab)=1$
\item Which means that for all points 
\[
d_p(\mathbf{w}^T\mathbf{x}_p+b)\geqslant{}1
\]
\end{itemize}
\end{frame}


% 22
\begin{frame}[c]
\frametitle{Maximum margin constrained optimization problem}
\begin{itemize}
\item Then the maximum margin optimization becomes
\[
\begin{array}{c}
{\argmax}_{\mathbf{w},b}\dfrac{1}{||\mathbf{w}||} \min_pd_py(\mathbf{x}_p)\\
= {\argmax}_{\mathbf{w},b}\dfrac{1}{||\mathbf{w}||}
\end{array}
\]
\item With the constraints $d_p(\mathbf{w}^T\mathbf{x}_p+b)\geqslant{}1$
\end{itemize}
\end{frame}

% 23
\begin{frame}[c]
\frametitle{Maximum margin constrained optimization problem}
\begin{itemize}
\item Which is equivalent to \\
$\argmin_{\mathbf{w},b}$ $\dfrac{1}{2}||\mathbf{w}||^2$, subject to $d_p(\mathbf{w}^T\mathbf{x}_p+b)\geqslant{}1$
\item This is a well studied type of problem
\begin{itemize}
\item A quadratic program with linear inequality constraints
\end{itemize}
\end{itemize}
\end{frame}



% 24
\begin{frame}[c]
\frametitle{Detour: Lagrange multipliers solve constrained optimization problems}
\begin{itemize}
\item Want to maximize a function $f(x_1,x_2)$
\item Subject to the equality constraint $g(x_1,x_2)= 0$
\item Could solve $g(x_1,x_2)= 0$ for $x_1$ in terms of $x_2$
\begin{itemize}
\item But that is hard to do in general (i.e., on computers)
\end{itemize}
\item Or could use Lagrange multipliers
\begin{itemize}
\item Which are easier to use in general (i.e., on computers)
\end{itemize}
\end{itemize}
\end{frame}


% 25
\begin{frame}[c]
\frametitle{Lagrange multipliers with general $\mathbf{x}$}
\begin{itemize}
\item In general, we can write
\[
\max\nolimits_x f(\mathbf{x})~\text{subject~to}~g(\mathbf{x})=0
\]
\item Constraint $g(\mathbf{x})= 0$ defines a $D‚àí 1$ dimensional surface for $D$ dimensional $\mathbf{x}$
\end{itemize}
\end{frame}


% 26
\begin{frame}[c]
\frametitle{Example: Maximize $f(\mathbf{x})=1-x_1^2-x_2^2$ subject to $g(\mathbf{x})=x_1+x_2-1=0$}
\begin{center}
\includegraphics[width=0.7\linewidth]{fig8/lec826.jpg}
\end{center}
\end{frame}


% 27
\begin{frame}[c]
\frametitle{Gradients of $g$ and $f$ are orthogonal to surface at solution point}
\begin{center}
\includegraphics[width=0.65\linewidth]{fig8/lec827.jpg}
\end{center}
\end{frame}



% 28
\begin{frame}[c]
\frametitle{Gradients of $g$ and $f$ are orthogonal to surface at maximum of $f$}
\begin{itemize}
\item For $g$ because on all points on the surface $g(\mathbf{x})=0$
\item For $f$ because if it wasn't, you could move along the surface in the direction of the gradient to find a better maximum
\item Thus $\nabla f$ and $\nabla g$ are (anti-)parallel
\item And there must exist a scalar $\lambda$ such that
\[
\nabla f+\lambda \nabla g=0
\]
\end{itemize}
\end{frame}

% 29
\begin{frame}[c]
\frametitle{The Lagrangian function captures the constraints on ùë•ùë• and on the gradients}
\[
L(\mathbf{x},\lambda)=f(\mathbf{x})+\lambda g(\mathbf{x})
\]
\begin{itemize}
\item Setting gradient of $L$ with respect to $\mathbf{x}$ to 0 gives
\[
\nabla f+\lambda \nabla g=0
\]
\item Setting partial of $L$ with respect to $\lambda$ to 0 gives
\[
g(\mathbf{x})=0
\]
\item Thus stationary points of $L$ solve the constrained optimization problem
\end{itemize}
\end{frame}



% 30
\begin{frame}[c]
\frametitle{Example: Maximize $f(\mathbf{x})=1-x_1^2-x_2^2$ subject to $g(\mathbf{x})=x_1+x_2-1=0$}
\begin{center}
\includegraphics[width=0.65\linewidth]{fig8/lec830.jpg}
\end{center}
\end{frame}


% 31
\begin{frame}[c]
\frametitle{Example: Maximize $f(\mathbf{x})=1-x_1^2-x_2^2$ subject to $g(\mathbf{x})=x_1+x_2-1=0$}
\begin{itemize}
\item So the Lagrangian function is
\[
\begin{array}{r@{~}l}
L(\mathbf{x},\lambda)=&f(\mathbf{x})+\lambda g(\mathbf{x})\\
=&1-x_1^2-x_2^2+\lambda(x_1+x_2-1)
\end{array}
\]
\item The conditions for $L$ to be stationary are
\[
\begin{array}{l@{~}l}
\partial L/\partial x_1=-2x_1+\lambda=0\\
\partial L/\partial x_2=-2x_2+\lambda=0\\
\partial L/\partial \lambda=x_1+x_2-1=0\\
\end{array}
\]
\item Can solve to find $\lambda=1,x_1=x_2=\dfrac{1}{2}$
\end{itemize}
\end{frame}


% 32
\begin{frame}[c]
\frametitle{Lagrange multipliers can also be used with inequality constraints $g(\mathbf{x})\geqslant{}0$}
\begin{center}
\includegraphics[width=0.65\linewidth]{fig8/lec832.jpg}
\end{center}
\end{frame}


% 33
\begin{frame}[c]
\frametitle{Lagrange multipliers can also be used with inequality constraints $g(\mathbf{x})\geqslant{}0$}
\begin{itemize}
\item Now two kinds of solutions:
\item If $g(\mathbf{x})> 0$, then the solution only depends on $f(\mathbf{x})$
\begin{itemize}
\item Inside constraint surface with $\nabla f= 0$
\item Stationary point of $L(\mathbf{x},ùúÜ\lambda)$ with $\lambda= 0$
\item Constraint $g(\mathbf{x})$ is said to be inactive
\end{itemize}
\item If $g(x)= 0$, then same as before (with equality constraint)
\begin{itemize}
\item On boundary of constraint surface with $\nabla f$ pointing out
\item Stationary point of $L(\mathbf{x},\lambda)$ with $\lambda> 0$
\item Constraint $g(\mathbf{x})$ is said to be active
\end{itemize}

\end{itemize}
\end{frame}


% 34
\begin{frame}[c]
\frametitle{Lagrange multipliers can also be used with inequality constraints $g(\mathbf{x})\geqslant{}0$}
\begin{itemize}
\item In either case, $\lambda g(\mathbf{x})=0$
\item Thus maximizing $f(\mathbf{x})$ subject to $g(\mathbf{x})\geqslant{}0$ is obtained by optimizing $L(\mathbf{x},\lambda)$ \wrt $\mathbf{x}$ and $\lambda$ subject to
\[
\begin{array}{cc}
g(\mathbf{x})&\geqslant{}0\\
\lambda&\geqslant{}0\\
\lambda g(\mathbf{x})&=0
\end{array}
\]
\item These are known as the Karush-Kuhn-Tucker (KKT) conditions
\end{itemize}
\end{frame}


% 35
\begin{frame}[c]
\frametitle{Example: Maximize $f(\mathbf{x})=1-x_1^2-x_2^2$ subject to $g(\mathbf{x})=x_1+x_2-1\geqslant{}0$}
\begin{center}
\includegraphics[width=0.65\linewidth]{fig8/lec835.jpg}
\end{center}
\end{frame}


% 36
\begin{frame}[c]
\frametitle{Example: Maximize $f(\mathbf{x})=1-x_1^2-x_2^2$ subject to $g(\mathbf{x})=x_1+x_2-1\geqslant{}0$}
\begin{itemize}
\item So the Lagrangian function is 
\[
L(\mathbf{x},\lambda)=1-x_1^2-x_2^2+\lambda(x_1+x_2-1)
\]
\item The conditions for $L$ to be stationary are 
\[
\begin{array}{r@{~}l}
\partial L/\partial x_1=-2x_1+\lambda=0\\
\partial L/\partial x_2=-2x_2+\lambda=0\\
\partial L/\partial \lambda=x_1+x_2-1=0\\
\end{array}
\]
\item Can solve to find $\lambda=1,x_1=x_2=\dfrac{1}{2}$
\begin{itemize}
\item Which still satisfies KKT conditions
\end{itemize}
\end{itemize}
\end{frame}


% 37
\begin{frame}[c]
\frametitle{Example: Maximize $f(\mathbf{x})=1-x_1^2-x_2^2$ subject to $g(\mathbf{x})=-x_1-x_2+1\geqslant{}0$}
\begin{center}
\includegraphics[width=0.65\linewidth]{fig8/lec837.jpg}
\end{center}
\end{frame}


% 38
\begin{frame}[c]
\frametitle{Example: Maximize $f(\mathbf{x})=1-x_1^2-x_2^2$ subject to $g(\mathbf{x})=-x_1-x_2+1\geqslant{}0$}
\begin{itemize}
\item So the Lagrangian function is 
\[
L(\mathbf{x},\lambda)=1-x_1^2-x_2^2+\lambda(-x_1-x_2+1)
\]
\item The conditions for $L$ to be stationary are 
\[
\begin{array}{r@{~}l}
\partial L/\partial x_1=-2x_1-\lambda=0\\
\partial L/\partial x_2=-2x_2-\lambda=0\\
\partial L/\partial \lambda=x_1+x_2-1=0\\
\end{array}
\]
\item Can solve to find $\lambda=-1$
\begin{itemize}
\item which does not satisfy KKT condition $\lambda \geqslant{}0$
\end{itemize}
\item Instead use unconstrained solution $x_1=x_2=0$
\begin{itemize}
\item which does satisfy KKT conditions
\end{itemize}
\end{itemize}
\end{frame}


% 39
\begin{frame}[c]
\frametitle{Multiple constraints each get their own Lagrange multiplier}
\begin{itemize}
\item Maximize $f(\mathbf{x})$ subject to $g_i(\mathbf{x})= 0$ and $h_j(x)\geqslant{}0$
\item Leads to the Lagrangian function
\[
L(\mathbf{x,\lambda,\mu})=f(\mathbf{x})+\sum_i \lambda_ig_i(\mathbf{x})+\sum_ju_jh_j(\mathbf{x})
\]
\item Still solve for $\nabla L(\mathbf{x,\lambda,\mu})= 0$
\item Trickier in general to figure out which $h_j(\mathbf{x})$ constraints should be active
\end{itemize}
\end{frame}



% 40
\begin{frame}[c]
\frametitle{Minimizing $f(\mathbf{x})$ with an inequality constraint requires a slightly different Lagrange} 
\begin{itemize}
\item Minimize \wrt $\mathbf{x}$
\[
L(\mathbf{x},\lambda)=f(\mathbf{x})-\lambda g(\mathbf{x})
\]
\item Still subject to
\[
g(\mathbf{x})\geqslant{}0
\]
\end{itemize}
\end{frame}


% 41
\begin{frame}[c]
\frametitle{Summary of Lagrange multipliers with multiple inequality constraints}
\begin{itemize}
\item Goal: maximize $f(\mathbf{x})$ subject to $g_i(\mathbf{x})\geqslant{} 0$
\item Write down Lagrangian function
\[
L(\mathbf{x},\lambda)=f(x)+\sum_i\lambda_ig_i(\mathbf{x})
\]
\item Find points where $\nabla L(\mathbf{x},\lambda)= 0$
\item Keep points that satisfy constraints $g_x(\mathbf{x})\geqslant{}0$
\item Figure out which KKT conditions should be active
\begin{itemize}
\item Don't need to try all $2^I$ combinations for SVMs
\item Because $f(\mathbf{x})$ and $g(\mathbf{x})$ form a ``quadratic program''
\end{itemize}
\end{itemize}
\end{frame}

% 42
\begin{frame}[c]
\frametitle{Back to SVMs: Maximum margin solution is a fixed point of the Lagrangian function}
\begin{itemize}
\item Recall, the maximum margin hyperplane is

$\argmin_{\mathbf{w},b} \dfrac{1}{2}||\mathbf{w}||^2$ subject to $d_p(\mathbf{w}^T \mathbf{x}_p+b)\geqslant{}1$
\begin{itemize}
\item Minimization of a quadratic function subject to multiple linear inequality constraints
\end{itemize}
\item Will use Lagrange multipliers, $a_p$, to write Lagrangian function

$$L(\mathbf{w},b,\mathbf{a})=\dfrac{1}{2}||\mathbf{w}||^2-\sum_{p}a_p(d_p( \mathbf{w} ^T\mathbf{x}_p+b)-1)$$

\item Note that $\mathbf{x}_p$ and $d_p$ are fixed for the optimization
\end{itemize}
\end{frame}




\begin{frame}
\begin{center}
\chuhao Thank you! %\fontspec{LHANDW.TTF}
\end{center}
\end{frame}
\end{document}
